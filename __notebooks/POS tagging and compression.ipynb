{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6215d5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:50:24.814890Z",
     "start_time": "2022-01-23T16:50:23.128232Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path = [\"/Users/dchaplinsky/Projects/lang-uk/lang.org.ua/languk/\"] + sys.path\n",
    "os.environ[\"DJANGO_SETTINGS_MODULE\"] = \"languk.settings.dev\"\n",
    "import django\n",
    "\n",
    "django.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d31865",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:50:26.867595Z",
     "start_time": "2022-01-23T16:50:26.772447Z"
    }
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from django.conf import settings\n",
    "from corpus.mongodb import db\n",
    "from corpus.udpipe_model import Model as UDPipeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50e5bbff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:52:25.893777Z",
     "start_time": "2022-01-23T16:50:28.143742Z"
    }
   },
   "outputs": [],
   "source": [
    "model = UDPipeModel(settings.UDPIPE_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20331227",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T00:46:49.027343Z",
     "start_time": "2022-01-25T00:46:49.005466Z"
    }
   },
   "outputs": [],
   "source": [
    "test_doc = db.fiction.find_one({\"_id\": \"e0555ead86335ee058bf2eef5758839f0f124ee7\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c09b44f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T00:46:49.961864Z",
     "start_time": "2022-01-25T00:46:49.955561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e0555ead86335ee058bf2eef5758839f0f124ee7'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc[\"_id\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "353d55c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T00:55:19.235591Z",
     "start_time": "2022-01-25T00:55:19.046386Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"decompress\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from corpus.ud_converter import (\n",
    "    DECOMPRESS_UPOS_MAPPING,\n",
    "    DECOMPRESS_FEATURES_MAPPING,\n",
    "    DECOMPRESS_FEATURE_VALUES_MAPPING,\n",
    "    grouper\n",
    ")\n",
    "\n",
    "\n",
    "def unpack_values(param_name, s):\n",
    "    def _unpack_value(v):\n",
    "        if param_name == \"ud_postags\":\n",
    "            try:\n",
    "                return DECOMPRESS_UPOS_MAPPING[v]\n",
    "            except KeyError:\n",
    "                logger.warning(\n",
    "                    f\"Cannot find the upos '{v}' in the mapping, skipping it for now\"\n",
    "                )\n",
    "                return \"UNK\"\n",
    "\n",
    "        elif param_name == \"ud_features\":\n",
    "            res = []\n",
    "\n",
    "            for c_cat, c_val in grouper(v, 2):\n",
    "                try:\n",
    "                    cat = DECOMPRESS_FEATURES_MAPPING[c_cat]\n",
    "                except KeyError:\n",
    "                    logger.warning(\n",
    "                        f\"Cannot find the feature '{c_cat}' in the mapping, skipping it for now\"\n",
    "                    )\n",
    "                    cat = \"UNK\"\n",
    "\n",
    "                try:\n",
    "                    val = DECOMPRESS_FEATURE_VALUES_MAPPING[cat][c_val]\n",
    "                except KeyError:\n",
    "                    logger.warning(\n",
    "                        f\"Cannot find the value '{c_val}' for the feature '{cat}' in the mapping, skipping it for now\"\n",
    "                    )\n",
    "                    \n",
    "                    val = \"UNK\"\n",
    "\n",
    "                res.append((cat, val))\n",
    "            return OrderedDict(res)\n",
    "\n",
    "        else:\n",
    "            return v\n",
    "\n",
    "    if param_name == \"ud_postags\":\n",
    "        return [[_unpack_value(w) for w in l] for l in s.split(\"\\n\")]\n",
    "    else:\n",
    "        return [[_unpack_value(w) for w in l.split(\" \")] for l in s.split(\"\\n\")]\n",
    "\n",
    "\n",
    "def decompress(tokens=None, ud_lemmas=None, ud_features=None, ud_postags=None):\n",
    "    params = locals()\n",
    "\n",
    "    assert any(\n",
    "        map(lambda x: x is not None, params.values())\n",
    "    ), \"at least one param should be not None\"\n",
    "\n",
    "    zipped = {}\n",
    "\n",
    "    for param_name, param_value in params.items():\n",
    "        if param_value is not None:\n",
    "#             if param_name == \"tokens\":\n",
    "#                 # TODO: validate if this workaround can be properly fixed\n",
    "#                 param_value = param_value.strip()\n",
    "            zipped[param_name] = unpack_values(param_name, param_value)\n",
    "    \n",
    "\n",
    "    sentences_length = set(map(len, zipped.values()))\n",
    "    assert len(sentences_length) == 1, f\"Text contains different number of sentences: {sentences_length}\"\n",
    "\n",
    "    res = []\n",
    "    param_names = list(zipped.keys())\n",
    "    param_values = list(zipped.values())\n",
    "\n",
    "    for sent in zip(*param_values): \n",
    "        word_length = set(map(len, sent))\n",
    "\n",
    "        assert len(sentences_length) == 1, f\"Text contains different number of words in sentence: {sent}\"\n",
    "\n",
    "        res.append(\n",
    "            [OrderedDict(zip(param_names, word_info)) for word_info in zip(*sent)]\n",
    "        )\n",
    "\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "_ = decompress(\n",
    "    tokens=test_doc[\"nlp\"][\"text\"][\"tokens\"],\n",
    "    ud_lemmas=test_doc[\"nlp\"][\"text\"][\"ud_lemmas\"],\n",
    "    ud_features=test_doc[\"nlp\"][\"text\"][\"ud_features\"],\n",
    "    ud_postags=test_doc[\"nlp\"][\"text\"][\"ud_postags\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe3c267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "412124bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T13:54:26.739514Z",
     "start_time": "2022-01-25T13:54:26.640187Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot find lemmatized version of field title in the document d6c6f1f0fc7c6bce04bd2a3b84d92a2d5f9bddb0\n",
      "Cannot find lemmatized version of field text in the document d6c6f1f0fc7c6bce04bd2a3b84d92a2d5f9bddb0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ufal.udpipe import Sentence\n",
    "\n",
    "from deepdiff import DeepDiff\n",
    "\n",
    "def compare_article(article):\n",
    "    for f in [\"title\", \"text\"]:\n",
    "        if f not in article[\"nlp\"]:\n",
    "            logger.warning(f\"Cannot find field {f} in the document {article['_id']}\")\n",
    "            continue\n",
    "\n",
    "        if \"tokens\" not in article[\"nlp\"][f]:\n",
    "            logger.warning(\n",
    "                f\"Cannot find tokenized version of field {f} in the document {article['_id']}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        if \"ud_lemmas\" not in article[\"nlp\"][f]:\n",
    "            logger.warning(\n",
    "                f\"Cannot find lemmatized version of field {f} in the document {article['_id']}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        if \"ud_features\" not in article[\"nlp\"][f]:\n",
    "            logger.warning(\n",
    "                f\"Cannot find udpipe features of field {f} in the document {article['_id']}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        if \"ud_postags\" not in article[\"nlp\"][f]:\n",
    "            logger.warning(\n",
    "                f\"Cannot find udpipe postags of field {f} in the document {article['_id']}\"\n",
    "            )\n",
    "            continue\n",
    "            \n",
    "\n",
    "        decompressed_result = decompress(\n",
    "            tokens=article[\"nlp\"][f][\"tokens\"],\n",
    "            ud_lemmas=article[\"nlp\"][f][\"ud_lemmas\"],\n",
    "            ud_features=article[\"nlp\"][f][\"ud_features\"],\n",
    "            ud_postags=article[\"nlp\"][f][\"ud_postags\"],\n",
    "        )\n",
    "        udpipe_res = []\n",
    "\n",
    "        for s in article[\"nlp\"][f][\"tokens\"].split(\"\\n\"):\n",
    "            tok_sent = Sentence()\n",
    "            for w in s.split(\" \"):\n",
    "                tok_sent.addWord(w)\n",
    "\n",
    "            model.tag(tok_sent)\n",
    "\n",
    "            udpipe_res.append(\n",
    "                [\n",
    "                    {\n",
    "                        \"tokens\": w.form,\n",
    "                        \"ud_lemmas\": w.lemma,\n",
    "                        \"ud_postags\": w.upostag,\n",
    "                        \"ud_features\": OrderedDict(\n",
    "                            (f.split(\"=\") for f in w.feats.split(\"|\"))\n",
    "                        ) if w.feats else OrderedDict(),\n",
    "                    }\n",
    "                    for w in tok_sent.words[1:]\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        comparison = udpipe_res == decompressed_result\n",
    "        if not comparison:\n",
    "            print(f\"Comparing {f} for the {article['_id']}: {comparison}\")\n",
    "        \n",
    "        if not comparison:\n",
    "            with open(\"udpipe_res.json\", \"w\") as fp_out:\n",
    "                json.dump(udpipe_res, fp_out, indent=4, ensure_ascii=False, sort_keys=True)\n",
    "            with open(\"decompressed_result.json\", \"w\") as fp_out:\n",
    "                json.dump(decompressed_result, fp_out, indent=4, ensure_ascii=False, sort_keys=True)\n",
    "\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "compare_article(test_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0a68265",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T15:09:26.205317Z",
     "start_time": "2022-01-25T13:54:33.326892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a7e375230a4e1f86560c1976eacc5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot find field title in the document 3eafcb11ef77e173a6c2cebdb49ca58baad12c71\n",
      "Cannot find field text in the document 3eafcb11ef77e173a6c2cebdb49ca58baad12c71\n",
      "Cannot find lemmatized version of field title in the document d6c6f1f0fc7c6bce04bd2a3b84d92a2d5f9bddb0\n",
      "Cannot find lemmatized version of field text in the document d6c6f1f0fc7c6bce04bd2a3b84d92a2d5f9bddb0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'nlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5n/gbgb43lj3sl83_ddvbpytvk80000gn/T/ipykernel_56362/3293643357.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtest_doc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcompare_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Comparison failed on {test_doc['_id']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/5n/gbgb43lj3sl83_ddvbpytvk80000gn/T/ipykernel_56362/1617336058.py\u001b[0m in \u001b[0;36mcompare_article\u001b[0;34m(article)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompare_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nlp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cannot find field {f} in the document {article['_id']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'nlp'"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "for test_doc in tqdm(db.fiction.find()):\n",
    "    if not compare_article(test_doc):\n",
    "        print(f\"Comparison failed on {test_doc['_id']}\")\n",
    "        break\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b817566c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T13:16:15.211403Z",
     "start_time": "2021-12-15T13:16:15.207009Z"
    }
   },
   "outputs": [],
   "source": [
    "s = \"\"\"Звернімося просто до його творчої біографії яка починається ще на порозі юності збіркою 1910 року На білих островах і поки що доходить до книжок віршів виданих у 1957 1959 роках\n",
    "За підрахунком одного з критиків М. Рильського у цілому це складає більше 25 збірок оригінальних поезій і понад 250 тисяч рядків поетичних перекладів а до того слід додати ще численні статті і дослідження з історії літератури народної творчості театру багато публіцистичних виступів\n",
    "Початкове формування таланту М. Рильського припадає на роки 1907 1917\"\"\"\n",
    "\n",
    "s = \"так-сяк понад 250 тисяч\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5c8b23b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T13:17:57.505845Z",
     "start_time": "2021-12-15T13:17:57.498932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "так так ADV\n",
      "- - PUNCT\n",
      "сяк сяк ADV\n",
      "понад 250 понад 250 NUM\n",
      "тисяч тисяча NOUN\n"
     ]
    }
   ],
   "source": [
    "for ss in s.split(\"\\n\"):\n",
    "    tokenized = model.tokenize(ss)\n",
    "    for tok_sent in tokenized:\n",
    "        sent_lemmas = []\n",
    "        sent_postags = []\n",
    "        sent_features = []\n",
    "\n",
    "        model.tag(tok_sent)\n",
    "\n",
    "        for w in tok_sent.words[1:]:\n",
    "            print(w.form, w.lemma, w.upostag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "966b5d48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T13:17:56.003289Z",
     "start_time": "2021-12-15T13:17:56.000276Z"
    }
   },
   "outputs": [],
   "source": [
    "model.tokenizer = model.model.newTokenizer(model.model.TOKENIZER_PRESEGMENTED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6c19e82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T13:23:09.513741Z",
     "start_time": "2021-12-15T13:23:09.509577Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = model.model.newTokenizer(model.model.TOKENIZER_NORMALIZED_SPACES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03cd238d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T13:25:21.790161Z",
     "start_time": "2021-12-15T13:25:21.784103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "понад 250\n",
      "понад 250\n",
      "понад 250\n"
     ]
    }
   ],
   "source": [
    "import ufal.udpipe\n",
    "\n",
    "\n",
    "for tok in [model.model.TOKENIZER_PRESEGMENTED, model.model.TOKENIZER_NORMALIZED_SPACES, model.model.TOKENIZER_RANGES]:\n",
    "    tokenizer = model.model.newTokenizer(tok)\n",
    "    tokenizer.setText(s)\n",
    "\n",
    "    error = ufal.udpipe.ProcessingError()\n",
    "    sentences = []\n",
    "\n",
    "    sentence = ufal.udpipe.Sentence()\n",
    "    while tokenizer.nextSentence(sentence, error):\n",
    "        sentences.append(sentence)\n",
    "        sentence = ufal.udpipe.Sentence()\n",
    "\n",
    "    print(sentences[0].words[4].form)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb65b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
