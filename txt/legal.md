Ми назбирали та упорядкували значний (більше 6ГБ) обсяг текстів українських періодичних видань й потрохи розширюємо наш архив.

Саме ці тексти ми використовуємо для обчислення наших Word Embeddings. На жаль, ліцензійні обмеження деяких видань не дозволяють нам публікувати ці тексти без змін.

Щоб надати публічний доступ до цього масиву даних ми зробили токенізацію усіх текстів на речення та слова, а потім перемішали речення у випадковому порядку. Таким чином, будь хто зможе використати ці тексти для обчислення статистичних моделей що працюють на рівні речень. Ми також публікуємо лематизовану версію цих текстів окремим архивом. Для токенізації та лематизації текстів ми використовували пакет nlp-uk від Андрія Рисіна та ініціативи БРУК
