# Базовий токенізатор

Наш токенізатор працює за трирівневим алгоритмом, який легко відтворити.

- перший рівень — розбивка тексту на абзаци
- другий рівень — розбивка абзацу на окремі речення
- третій рівень — розбивка речень на токени

Відповідно, результатом роботи алгоритму є список списків списків токенів.

Токенізатор доступний для наступних мов:

- Python: `pip install tokenize-uk`
- Lisp: `(load "tokenize-uk.lisp")` (файл можна завантажити [тут](https://raw.githubusercontent.com/vseloved/tokenize-uk/master/tokenize-uk.lisp))


## Стандартний алгоритм токенізації

1. Розбивка тексту на абзаци по символу `Newline`
2. Розбивка тексту на речення за настуним алгоритмом

   - Розбивка на токени по регекспу `[^\s]+`
   - Якщо токен закінчується на один з розділових знаків `.!?…»`
   (за виключенням випадків, коли цей токен
   починається з відкриваючої дужки,
   є однією великою літерою
   або входить в перелік загальновживаних абревіатур з крапкою - див. нижче),
   а наступний токен починається з великої літери, то це границя речення.

3. Розбивка речень на токени по наступному регулярному виразу:

        \w+://(?:[a-zA-Z]|[0-9]|[$-_@.&+])+
        |[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+
        |[0-9]+-[а-яА-ЯіїІЇ'’`]+
        |[+-]?[0-9](?:[0-9,.-]*[0-9])?
        |[\w](?:[\w'’`-]?[\w]+)*
        |\w.(?:\w.)+\w?
        |["#$%&*+,/:;<=>@^`~…\\(\\)⟨⟩{}\\[\\|\\]‒–—―«»“”‘’'№]
        |[.!?]+
        |-+
